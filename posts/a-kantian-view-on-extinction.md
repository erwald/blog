---
layout: layouts/post.njk
title: A Kantian View on Extinction
date: 2022-07-16
tags: post
---

# A Kantian View on Human Extinction

To a utilitarian, whether or not human extinction is bad depends on whether we expect the long-term future to be positive or negative. Whatever it is, there'll be a lot of it. Maybe we don't outlive the century. But if we do, we may survive for 1-10 million years, as many other species do (Ord 2020, 218–19). Or we may survive for several hundred million years, as some other species do (Ord 2020, 220). Or with some luck we may survive for 1+ billion years, or many billions of years if we leave the planet or even the galaxy (Ord 2020, 221–23). If we think the future is good, we should think it's very good; if bad, we should dread it like we dread our own deaths.

I think utilitarianism is fairly plausible and give it some weight. I think Kantianism is more plausible and give it more weight. What does Kantianism say about human extinction?

I know some people who'd say: Isn't human extinction just obviously bad? It'd probably be brought about by the worst catastrophe that's ever befallen humankind. The Black Plague was horrifying, but it didn't even get close to driving us to extinction; it'd take something far worse to do that.

I know some other people who'd say: Isn't human extinction just obviously good? Humans cause so much suffering. We murder, deceive, abuse and bully one another all the time. Every year we kill ~70 billion chicken, sheep, goats and cattle;[^1] if we die, at least it means factory farming is a thing of the past.

But this post is not about people dying. It's about the death of humanity. Sure, each person's death is bad, and _perhaps_ also good (to the extent that they would've caused harm). I'm asking you to ignore those things and ask yourself: How much worse than the death of everyone except a minimally sustainable population is the death of _literally everyone_? How much worse (or better) is the death of all 8 billion people[^2] than the death of, say, 7,999,900,000 people? Is it only 0.001% worse? It probably would take a terrible catastrophe to bring human extinction about, but that no more means extinction is bad than the tragedy of the Shoah means there's no value in Paul Celan's poetry.

![img]({{ '/img/scientist_1.jpeg' | url }})

## Summary

The deontologist in me thinks human extinction would be very bad for three reasons:

1. We'd be failing in our duty to humanity itself (55% confidence).
2. We'd be failing in our duty to all those who have worked for a better future (70% confidence).
3. We'd be failing in our duty to those wild animals whose only hope for better lives rests on future human technology (35% confidence).

## Letting Down Humanity

Korsgaard (2018) makes roughly the following argument. We humans (unlike the other animals) think of ourselves as a collective agent, a "species-being". (It makes sense to ask, for example, "Will we colonise the galaxy?" and thereby refer to a multi-generational project in which all humans have a stake.) Claims about rights (says Kant) only make sense in a political community (which is kind of like a collective agent); the ultimate political community comprises the entire human species, so if we make claims of rights, we're committed to considering humanity as a collective agent. We humans (again unlike the other animals) are also aware of the grounds for our (potential) beliefs and actions; this allows us to reason _normatively_. Because we can reason normatively, we evaluate ourselves based on our actions, based on whether we act in a way that anyone can rationally endorse. That makes the ends that everyone share (the species-being's ends) especially important. Therefore, human extinction is especially bad (because all those important species-being-y actions would be thwarted).

My crude interpretation of the argument is something like, "It's important for us to think that we're good people. Doing good (in deontology) means taking good actions. Good actions are (roughly speaking) those that follow a maxim that any human can rationally endorse. Therefore, acting according to a maxim that benefits (and _not_ acting according to one that harms) the whole of humanity is good. But if humans go extinct, all those good humanity-promoting actions – at least those that were oriented towards the future – are thwarted."

Daniel Kokotajlo recently wrote a deep and lucid post [illustrating Kant's ethics with a "decision theory app store"](https://www.lesswrong.com/posts/byMNKEXBn4RTcaaa6/immanuel-kant-and-the-decision-theory-app-store). Allow me to quote at length:

> Imagine an ideal competitive market for advice-giving AI assistants. Tech companies code them up and then you download them for free from the app store. There is AlphaBot, MetaBot, OpenBot, DeepBot ...
>
> When installed, the apps give advice. Specifically they scan your brain to extract your credences and values/utility function, and then they tell you what to do. You can follow the advice or not.
>
> [...]
>
> Now, what's it like to _be_ one of these hyper-sophisticated advice bots? You are sitting there in your supercomputer getting all these incoming requests for advice, and you are dispensing advice like the amazing superhuman oracle you are, and you are also reflecting a bit about how to improve your overall advice-giving strategy ...
>
> You are facing a massive optimization problem. You shouldn't just consider each case in isolation; [...] you can sometimes do better by coordinating your advice across cases. But it's also not quite right to say you want to maximize total utility across all your users; if your advice predictably screwed over some users to benefit others, those users wouldn't take your advice, and then the benefits to the other users wouldn't happen, and then you'd lose market share to a rival bot that was just like you except that it didn't do that and thus appealed to those users.
>
> Kant says:
>
> "Look, it's complicated, and despite me being the greatest philosopher ever I don't know all the intricacies of how it'll work out. But I can say, at a high level of abstraction: The hyper-sophisticated advice bots are basically legislating laws for all their users to follow. They are the exalted Central Planners of a society consisting of their users. And so in particular, the best bot, the optimal policy, the one we call Instrumental Rationality, does this. And so in particular if you are trying to think about how to be rational, if you are trying to think about what the rational thing to do is, you should be thinking like this too – you should be thinking like a central planner optimizing the behavior of all rational beings, legislating laws for them all to follow."

Thinking about it this way, I think it's clear that such a central planner would try really hard not to legislate laws that increase the risk of human extinction. Because not only could this predictably screw over _everyone_, but by causing our extinction it would completely undermine itself; should we die, it'll have lost its reason to exist.

## Letting Down Those Who Came Before

In making her argument, Korsgaard references Scheffler (2013), which points out that many of our long-term goals would become meaningless if we were the last generation of humans. In a [review of _The Precipice_](https://archive.ph/UnOsH#selection-2243.0-2249.413), Jim Holt writes:

> [The prospect of imminent human extinction] would be 'profoundly depressing' [according to Scheffler]. And the reason is that the meaning and value of our own lives depend on their being situated in an ongoing flow of generations. Humanity's extinction soon after we ourselves are gone would render our lives today in great measure pointless. Whether you are searching for a cure for cancer, or pursuing a scholarly or artistic project, or engaged in establishing more just institutions, a threat to the future of humanity is also a threat to the significance of what you do. True, there are some aspects of our lives – friendship, sensual pleasures, games – that would retain their value even in an imminent doomsday scenario. But our long-term, goal-oriented projects would be robbed of their point.

You can look at your own goals, projects and aspirations and see this as a selfish reason to not want humanity to go extinct. You can look at others' goals, projects and aspirations and see that as an altruistic reason to avoid human extinction.

![img]({{ '/img/scientist_2.jpeg' | url }})

Most people who have ever worked to attain long-term goals, have died. I [find it plausible]({{ '/posts/the-atemporal-franz-kafka/' | url }}) that we can have duties to dead people, and therefore think Scheffler's idea is one reason to avoid human extinction, though I feel like it's a relatively weak reason. Like, if I kill someone in the present, that's really mostly bad because that someone lost their life, and not because their parents and grandparents worked in vain to give them a good life. But it does feel like we'd be letting down all those who've worked so hard to make it happen should we fail to, I dunno, [completely eradicate polio](https://en.wikipedia.org/wiki/Polio_eradication) or something.

## Letting Down Wild Animals

This is more speculative, but my thinking here is roughly this:

1. Wild animals mostly live really, really hard lives.[^3] Right now countless animals face starvation ... Some of those who don't, don't only because others died to feed them ... This moment innumerable animals are feeling an aching desire to reproduce, but will never find a partner ... Some do find a partner, but see that partner or their offspring die to disease, starvation, predation ... Life in nature is full of this stuff.
2. I think we have a duty of beneficence towards these animals. We should help them get what's good for them, and avoid the bad.
3. We don't really know how to help them now, but I think we can find ways of doing so in future.
4. We can't help them if we're dead. If we die, maybe all life there ever is on earth is doomed to a billion-year-long zero-sum struggle for survival. How depressing would that be!

Of course ... this is the case only if we actually do end up transforming wild animal life for the better. It's possible that the future is one of humans inflicting terrible pain and suffering on wild animals, or at any rate leaving them alone for nature to do what it wants with them, in which case our continued survival doesn't look that good from the animals' point of view. It's also possible that even if we could, it would be _wrong_ for us to radically intervene in nature; I find that unlikely but haven't ruled it out.

When I think of extinction, I think of a remark that Nietzsche made about Arthur Schopenhauer. Pointing out that Schopenhauer seemed to have had no friends (except his poodles), he wrote something to the effect of, "And between zero and one lies infinity."

## References

<style>.csl-entry{text-indent: -2em; margin-left: 2em;}</style><div class="csl-bib-body">
  <div class="csl-entry">Bostrom, Nick. 2004. “Golden.”</div>
  <div class="csl-entry">Korsgaard, Christine M. 2018. “Species-Being and the Badness of Extinction and Death.” <i>Zeitschrift Für Ethik Und Moralphilosophie</i> 1 (1): 143–62. https://doi.org/10.1007/s42048-018-0002-3</div>
  <div class="csl-entry">Ord, Toby. 2020. <i>The Precipice: Existential Risk and the Future of Humanity</i>. Hachette Books.</div>
  <div class="csl-entry">Scheffler, Samuel. 2013. <i>Death and the Afterlife</i>. Oxford University Press.</div>
</div>

[^1]: Source: [Faunalytics](https://faunalytics.org/global-animal-slaughter-statistics-and-charts/). The vast majority of those who die are chickens, and the trend is really depressing across the board ...
[^2]: Right now we seem to be at roughly 7.96 billion people, but humour me and assume we're at exactly 8 billion.
[^3]: As usual Bostrom (2004) puts it better: "[W]e must not forget that most animals live in the wild, which is not exactly a walk in the park. Many humans look at nature from an aesthetic perspective and think in terms of biodiversity and the health of ecosystems, but forget that the animals that inhabit these ecosystems are individuals and have their own needs. Disease, starvation, predation, ostracism, and sexual frustration are endemic in so-called healthy ecosystems."
